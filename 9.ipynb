{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d85bb66-deb3-4ed0-a222-a16230772010",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:1\n",
    "Simple Linear Regression is a statistical technique used to model the relationship between two continuous variables. It aims to find a linear relationship between the dependent variable (also called the response variable or target variable) and one independent variable (also called the predictor variable or feature). The model assumes that the relationship between the variables can be approximated by a straight line equation: Y = β0 + β1*X, where Y is the dependent variable, X is the independent variable, β0 is the intercept (the value of Y when X is 0), and β1 is the slope (the change in Y for a unit change in X). The goal is to find the best-fitting line that minimizes the sum of squared differences between the actual and predicted values.\n",
    "\n",
    "Multiple Linear Regression:\n",
    "Multiple Linear Regression is an extension of simple linear regression that considers more than one independent variable to predict the dependent variable. It models the relationship between the dependent variable and two or more independent variables. The equation for multiple linear regression can be represented as: Y = β0 + β1X1 + β2X2 + ... + βn*Xn, where Y is the dependent variable, X1, X2, ..., Xn are the independent variables, and β0, β1, β2, ..., βn are the coefficients representing the intercept and slopes for each independent variable. The goal is to find the best-fitting hyperplane that minimizes the sum of squared differences between the actual and predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c4b070-89e2-44c0-8477-5e63dfddc961",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans:2\n",
    "The key assumptions of linear regression are as follows:\n",
    "\n",
    "Linearity: The relationship between the dependent variable and each independent variable is linear. In other words, the change in the dependent variable is proportional to the change in each independent variable.\n",
    "\n",
    "Independence: The observations in the dataset are independent of each other. There should be no autocorrelation or dependence between data points.\n",
    "\n",
    "Homoscedasticity: The variance of the residuals (the differences between observed and predicted values) should be constant across all levels of the independent variables. In simpler terms, the spread of residuals should be consistent throughout the range of predicted values.\n",
    "\n",
    "Normality of Residuals: The residuals should follow a normal distribution. This means that the errors (residuals) should be normally distributed with a mean of zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05be0db6-d87f-4f40-826e-baf46dca2d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:3\n",
    "n a linear regression model of the form Y = β0 + β1*X, the slope (β1) and intercept (β0) have specific interpretations:\n",
    "\n",
    "Intercept (β0): The intercept represents the value of the dependent variable (Y) when the independent variable (X) is 0. It is the predicted value of Y when X is not present or has no effect. In many cases, the intercept might not have a practical interpretation, especially if X cannot be zero in the real-world context.\n",
    "\n",
    "Slope (β1): The slope represents the change in the dependent variable (Y) for a one-unit change in the independent variable (X). It quantifies the rate of change in Y for each unit increase (or decrease) in X. The sign of the slope (positive or negative) indicates the direction of the relationship between X and Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102cdab7-bdd1-4346-aa97-4698a0e7b6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:4\n",
    "Gradient descent is an optimization algorithm used in machine learning to find the optimal values of the parameters of a model that minimize a given cost function. The goal of gradient descent is to iteratively update the parameters in the direction of the steepest descent of the cost function, leading to the convergence to the global or local minimum of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3452b4-69ee-4cd0-92e4-e826227649f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:5\n",
    "Multiple linear regression is an extension of simple linear regression that involves modeling the relationship between a dependent variable (response variable) and two or more independent variables (predictor variables). The multiple linear regression model assumes that the dependent variable is a linear combination of the independent variables, with each independent variable having its own coefficient (slope) in the equation.\n",
    "\n",
    "The equation for multiple linear regression can be represented as:\n",
    "\n",
    "Y = β0 + β1X1 + β2X2 + ... + βn*Xn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8217da6-f78d-4993-a217-39cb5a8a1776",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:6\n",
    "Multicollinearity is a phenomenon that occurs in multiple linear regression when two or more independent variables are highly correlated with each other. In other words, multicollinearity exists when there is a strong linear relationship between two or more predictor variables, making it difficult for the model to distinguish their individual effects on the dependent variable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
